# -*- coding: utf-8 -*-
"""7600_preprocessing_0829.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Mslhla0M7b0jiJcjpwSAFLJ-ZzwClxR
"""

import pandas as pd
import jieba

from google.colab import drive

drive.mount('/content/drive')  ## https://blog.csdn.net/sdaujz/article/details/106531632
                 ## https://blog.csdn.net/sdaujz/article/details/106531632

cd /content/drive/MyDrive/Capstone_Project/data

data = pd.read_csv("0827_Cecilia.csv")
data.head(3)

data['Article'][0]

# jiebaåˆ†è¯çš„demoã€å¯è·³è¿‡ç›´æ¥ä¸‹ä¸€æ­¥ã€‘
import jieba
# è½½å…¥è‡ªå®šä¹‰è¯åº“ï¼ˆæ¯”å¦‚ï¼šæ‰€æœ‰è‚¡ç¥¨åç§°/ä»£ç 
# jieba.load_userdict("./dict/jieba_dict.txt")

jieba.enable_paddle()# å¯åŠ¨paddleæ¨¡å¼ã€‚ 0.40ç‰ˆä¹‹åå¼€å§‹æ”¯æŒï¼Œæ—©æœŸç‰ˆæœ¬ä¸æ”¯æŒ

seg_list = jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦",use_paddle=True) # ä½¿ç”¨paddleæ¨¡å¼
print("Paddle Mode: " + '/'.join(list(seg_list)))

seg_list = jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # å…¨æ¨¡å¼

seg_list = jieba.cut("æˆ‘æ¥åˆ°åŒ—äº¬æ¸…åå¤§å­¦", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # ç²¾ç¡®æ¨¡å¼

seg_list = jieba.cut("ä»–æ¥åˆ°äº†ç½‘æ˜“æ­ç ”å¤§å¦")  # é»˜è®¤æ˜¯ç²¾ç¡®æ¨¡å¼
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("å°æ˜ç¡•å£«æ¯•ä¸šäºä¸­å›½ç§‘å­¦é™¢è®¡ç®—æ‰€ï¼Œååœ¨æ—¥æœ¬äº¬éƒ½å¤§å­¦æ·±é€ ")  # æœç´¢å¼•æ“æ¨¡å¼
print(", ".join(seg_list))

# ç¼ºå¤±å€¼å¤„ç†
# ç¼ºå¤±å€¼æŸ¥æ‰¾ä¸å¡«å……ï¼šhttps://blog.csdn.net/qq_28811329/article/details/82821971?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control
import math 
# åˆ é™¤å«æœ‰ç¼ºå¤±å€¼çš„è¡Œï¼šhttps://blog.csdn.net/Guo_ya_nan/article/details/81042882 æˆ–è€… https://www.delftstack.com/zh/howto/python-pandas/pandas-drop-rows-with-nan/#:~:text=Pandas%20%E4%BD%BF%E7%94%A8%20DataFrame.dropna%20%28%29%20%E6%96%B9%E6%B3%95%E5%8F%AA%E5%88%A0%E9%99%A4%E6%89%80%E6%9C%89%E5%88%97%E9%83%BD%E6%98%AF%20NaN%20%E5%80%BC%E7%9A%84%E8%A1%8C.%20%E5%AE%83%E5%8F%AA%E5%88%A0%E9%99%A4,%E6%88%91%E4%BB%AC%E5%9C%A8%20dropna%20%28%29%20%E6%96%B9%E6%B3%95%E4%B8%AD%E8%AE%BE%E7%BD%AE%20how%3D%27all%27%20%EF%BC%8C%E8%AE%A9%E8%AF%A5%E6%96%B9%E6%B3%95%E5%8F%AA%E5%9C%A8%E8%A1%8C%E7%9A%84%E6%89%80%E6%9C%89%E5%88%97%E5%80%BC%E9%83%BD%E6%98%AF%20NaN%20%E6%97%B6%E6%89%8D%E5%88%A0%E9%99%A4%E8%A1%8C%E3%80%82.
data_copy = data.copy()
data_copy.dropna(axis=0, how='any', inplace=True)
print("åˆ é™¤å‰ï¼š",len(data))
print("åˆ é™¤åï¼š",len(data_copy))
# åˆ é™¤ç›®æ ‡å€¼Yï¼ˆpct_chgï¼‰ç¼ºå¤±çš„è¡Œ
def is_not_nan(x):
  return not math.isnan(x)
data_copy = data_copy[data_copy['pct_chg'].map(is_not_nan)]
print("åˆ é™¤å‰ï¼š",len(data))
print("åˆ é™¤åï¼š",len(data_copy))

data = data_copy.reset_index()

#åˆ é™¤å›ºå®šbannerä½
for i in range(0,len(data)):
    if 'é‡‘éº’éºŸ' in data['Article'][i]:
        data['Article'][i] = data['Article'][i][40:9999999]

#åˆ é™¤Yï¼ˆpct_chgï¼‰ç¼ºå¤±çš„è¡Œ


def getJiebaList(x):
  # å¯¹å­—ç¬¦ä¸²'['string1','string2']'ç»“å·´åˆ†è¯Â¶ï¼Œå¾—åˆ°listï¼š[[string1 word],[string2 word]]
  result = []
  y = x.strip("[").strip("]").split(", ")
  for string in y:
    tmp = " ".join(jieba.cut(string.strip("\'").strip('\\u3000\\u3000').strip('\u3000\u3000'))).split()
    result.append(tmp)
  return result

def transListsToList(x):
  # å°†åŒ…å«å¤šä¸ªlistsçš„list xåˆå¹¶ä¸º1ä¸ªlist(å³ä¸€ç¯‡æ–‡ç« çš„è¯æ±‡å­˜åœ¨1ä¸ªlistä¸­)
  return [i for p in x for i in p]


data['doc_Jieba_List']=data.Article.apply(getJiebaList)
data['doc_Jieba']=data.doc_Jieba_List.apply(transListsToList)
data.head(1)

for i in range(len(data['Article'])):
  if 'æŠ•èµ„æ”¶ç›Š' in data['Article'][i]:
    print(data['doc_Jieba'][i],'\n')
    print(data['URL'][i])
    break

def get_custom_stopwords(stop_words_file):
    with open(stop_words_file) as f:
        stopwords = f.read()
    stopwords_list = stopwords.split('\n')
    custom_stopwords_list = [i for i in stopwords_list]
    return custom_stopwords_list

stop_words_file = './stopwords-master/å“ˆå·¥å¤§åœç”¨è¯è¡¨.txt'
stopwords_1 = get_custom_stopwords(stop_words_file)
stop_words_file = './stopwords-master/ä¸­æ–‡åœç”¨è¯è¡¨.txt'
stopwords_2 = get_custom_stopwords(stop_words_file)
stop_words_file = './stopwords-master/å››å·å¤§å­¦æœºå™¨æ™ºèƒ½å®éªŒå®¤åœç”¨è¯åº“.txt'
stopwords_3 = get_custom_stopwords(stop_words_file)
stop_words_file = './stopwords-master/ç™¾åº¦åœç”¨è¯è¡¨.txt'
stopwords_4 = get_custom_stopwords(stop_words_file)
stopwords=stopwords_1+stopwords_2+stopwords_3+stopwords_4
stopwords=list(set(stopwords))
#å»é‡å2318ä¸ªåœç”¨è¯

def clean_stopword(x):
    y=[]
    for i in x:
        if i not in stopwords:
            y.append(i)
    return y

data['doc_Jieba_2']=data.doc_Jieba.apply(clean_stopword)

# Selecting frequently-used wordsï¼šé€‰æ‹©å‡ºç°æ¬¡æ•°å¤§äºkappaçš„è¯æ±‡ï¼ˆkappaä¸ºè¶…å‚æ•°ï¼‰ï¼Œå¯¹åº”è®ºæ–‡ä¸­D__freq
## è®¡ç®—åŒ…å«è¯æ±‡içš„æ–‡ç« æ•°ï¼Œå­˜åœ¨å­—å…¸dict_word_ariticlesä¸­ï¼›ç„¶åè®¡ç®—æ¯ç¯‡æ–‡ç« çš„è¯æ±‡set set_word_ariticlesï¼Œ
dict_word_ariticles = {}
for article_list in data['doc_Jieba_2']:
  set_word_ariticles = set(article_list)
  for word in list(set_word_ariticles):
    if word in dict_word_ariticles:
      dict_word_ariticles[word] += 1
    else:
      dict_word_ariticles[word] = 1
# dict_word_ariticles


## é€‰æ‹©å‡ºç°æ¬¡æ•°å¤§äºkappaçš„è¯æ±‡ï¼ˆkappaä¸ºè¶…å‚æ•°ï¼‰
kappa = 20
frequent_dict = [k for k, v in dict_word_ariticles.items() if v > kappa]

# è¯åˆ°IDçš„æ˜ å°„, ä½¿å¾—æ¯ä¸ªè¯æœ‰ä¸€ä¸ªID
D__freq = dict(zip(frequent_dict, range(len(frequent_dict))))
# sorted(D__freq)
len(D__freq)

tmp_num = 0
for i in range(0,len(data)):
    if 'æ”¶ç›Š' in data['Article'][i]:
        tmp_num += 1
print(tmp_num)
tmp_num = 0
for i in range(0,len(data)):
    if 'æ”¶ç›Š' in data['doc_Jieba_2'][i]:
        tmp_num += 1
print(tmp_num)
print(dict_word_ariticles['æ”¶ç›Š'] + dict_word_ariticles['æ”¶ç›Šç‡'] + dict_word_ariticles['æŠ•èµ„æ”¶ç›Š'] + dict_word_ariticles['æ”¶ç›Šåˆ†é…'] + dict_word_ariticles['ç»æµæ”¶ç›Š'] + dict_word_ariticles['æ”¶ç›Šæƒ'] + dict_word_ariticles['ä½æ”¶ç›Š'])
for k,v in dict_word_ariticles.items():
  if 'æ”¶ç›Š' in k:
    print(k,'\n')
dict_word_ariticles['æ”¶ç›Š']

"""### Factor modelingã€todoã€‘
$ X = FB^T + U$

### Learning factors and idiosyncratic componentsã€todoã€‘
ï¼ˆ0ï¼‰æ±‚Xï¼š

ï¼ˆ1.1ï¼‰æŒ‡å®škï¼Œæ±‚Fï¼š$F=sqrt(n)*(XX^Tçš„æœ€å¤§çš„å‰kä¸ªç‰¹å¾å€¼ï¼‰$

ï¼ˆ1.2ï¼‰æ±‚å‡ºkï¼Œå†æ±‚F

ï¼ˆ2ï¼‰ æ ¹æ®Xå’ŒFï¼Œæ±‚Bï¼š$B=X^TF/n$

ï¼ˆ3ï¼‰æ ¹æ®Xï¼ŒFå’ŒBï¼Œæ±‚Uï¼š$U=X-FB^T$
"""

# sorted(D__freq.items(), key=lambda item:item[1])

# ï¼ˆ0ï¼‰æ±‚X
from collections import Counter
from itertools import chain
import numpy as np
documents = data['doc_Jieba_2']
def word_matrix(docs, dictionary, word_num, doc_num ):
    '''è®¡ç®—è¯é¢‘çŸ©é˜µï¼šè¡Œè¡¨ç¤ºæ–‡æ¡£ï¼Œåˆ—è¡¨ç¤ºè¯è¯­'''
    # è¯åˆ°IDçš„æ˜ å°„, ä½¿å¾—æ¯ä¸ªè¯æœ‰ä¸€ä¸ªID
    # dictionary = dict(zip(words, range(len(words))))
    #print(dictionary)
    # åˆ›å»ºä¸€ä¸ªç©ºçš„çŸ©é˜µ, è¡Œæ•°ç­‰äºæ–‡æ¡£æ•°, åˆ—æ•°ç­‰äºè¯æ•°
    matrix = np.zeros((doc_num,  word_num))
    # é€ä¸ªæ–‡æ¡£ç»Ÿè®¡è¯é¢‘
    for row in range(doc_num):  # row è¡¨ç¤ºçŸ©é˜µç¬¬å‡ è¡Œï¼Œå³ç¬¬å‡ ä¸ªæ–‡æ¡£
        # ç»Ÿè®¡è¯é¢‘
        count = Counter(docs[row])#å…¶å®æ˜¯ä¸ªè¯å…¸ï¼Œè¯å…¸å…ƒç´ ä¸ºï¼š{å•è¯ï¼šæ¬¡æ•°}ã€‚
        for word in count:
          if word in D__freq:
            # ç”¨wordçš„idè¡¨ç¤ºwordåœ¨çŸ©é˜µä¸­çš„åˆ—æ•°ï¼Œè¯¥æ–‡æ¡£è¡¨ç¤ºè¡Œæ•°ã€‚
            col= dictionary[word] # colè¡¨ç¤ºçŸ©é˜µç¬¬å‡ åˆ—ï¼Œå³ç¬¬å‡ ä¸ªå•è¯
            # æŠŠè¯é¢‘èµ‹å€¼ç»™çŸ©é˜µ
            matrix[row,col] = count[word]
    return matrix
 
X = word_matrix(documents, D__freq, len(D__freq), len(documents))
print("Word frequency matrix is:",X) ## [doc_num, word_num]

# ï¼ˆ1.1ï¼‰æŒ‡å®škï¼Œæ±‚Fï¼š$F=sqrt(n)*(XX^Tçš„æœ€å¤§çš„å‰kä¸ªç‰¹å¾å€¼ï¼‰$ã€‚ã€todoã€‘æ£€æŸ¥ä¸‹ç®—å¾—å¯¹ä¸å¯¹ ï¼›ã€todoã€‘è¿™é‡Œkæ˜¯è¶…å‚æ•°
# ã€å‚è€ƒï¼šhttps://blog.csdn.net/w450468524/article/details/54885773
import numpy as np
n = len(documents)
k = 10

eigenvalue, featurevector = np.linalg.eig(np.matmul(X,X.T))
sorted_indices = np.argsort(eigenvalue)
# ä¸Šé¢å¾—åˆ°çš„sorted_indiceså°±æ˜¯ç‰¹å¾å€¼æ’åºåçš„ç»“æœï¼Œå·§å¦™çš„æ˜¯è¿™é‡Œæ˜¯ç”¨æ•°ç»„ä¸‹æ ‡æ¥è¡¨ç¤ºçš„ï¼Œä¹Ÿå°±æ˜¯è¯´å…¶ä¸­å­˜æ”¾çš„æ˜¯ç‰¹å¾å€¼ç”±å°åˆ°å¤§çš„é¡ºåºæ’åºæ—¶å¯¹åº”çš„ä¸‹æ ‡[0, 4, 1, 3, 2]ï¼Œè€Œä¸æ˜¯ç›´æ¥å­˜æ”¾ç‰¹å¾å€¼ã€‚
# ä¸‹ä¸€æ­¥å°±æ˜¯å–å‰kå¤§çš„ç‰¹å¾å‘é‡äº†ï¼š
topk_evecs = featurevector[:,sorted_indices[:-k-1:-1]]

F = np.sqrt(n) * topk_evecs  ## [doc_num, k]
print("F = ",F)

# å¤æ•°ï¼š
# å®æ•°éƒ¨åˆ†å’Œè™šæ•°éƒ¨åˆ†éƒ½æ˜¯æµ®ç‚¹æ•°
# è™šæ•°éƒ¨åˆ†å¿…é¡»æœ‰jæˆ–J
# ä¸‹é¢æ˜¯äº›å¾—æ•°ï¼š
# 64.23+1j            4.34-8.5j            0.23-8.33j            1.23e-0.45+6.5e+0.83j              -1.23-3.5j            -0.34-0j
# å¤æ•°ä¸­çš„å†…å»ºå±æ€§

# ï¼ˆ1.2ï¼‰æ±‚å‡ºkï¼Œå†æ±‚Fï¼š

# ï¼ˆ2ï¼‰ æ ¹æ®Xå’ŒFï¼Œæ±‚Bï¼š$B=X^TF/n$
B = np.matmul(X.T,F) / n
print("B =",B)  ## [word_num, k]
# ï¼ˆ3ï¼‰æ ¹æ®Xï¼ŒFå’ŒBï¼Œæ±‚Uï¼š$U=X-FB^T$
U = X - np.matmul(F,B.T)
print("U =",U) ## [doc_num, word_num]

word_num , doc_num = len(D__freq), len(documents)
assert X.shape == (doc_num, word_num)
assert F.shape == (doc_num, k)
assert B.shape == (word_num, k)
assert U.shape == (doc_num, word_num)

"""### Learning conditional sentiment-charged words Sã€todoã€‘"""

## å¯¹äºYç¼ºå¤±å€¼ï¼Œå…ˆç”¨å¹³å‡æ•°å¡«å……ï¼Œä¹‹åç¡®å®šäº†Yï¼Œè¦åœ¨ä¸€å¼€å§‹å°±æŠŠåŒ…å«ç¼ºå¤±Yå€¼çš„dataçš„æ•´è¡Œæ•°æ®åˆ é™¤
data.open = data.open.fillna(data.open.mean())

data.open

# 1.æ ¹æ®Yå’ŒFåšçº¿æ€§å›å½’ï¼Œæ±‚æ®‹å·®Y_u
# å…ˆç”¨æ”¶ç›˜ä»·ï¼ˆopenï¼‰ä½œä¸ºY
from numpy import random
Y = data.open
# åˆ¤æ–­Fçš„æ¯ä¸ªå¤æ•°å…ƒç´ æ˜¯å¦ä¸ºå®æ•°ï¼š
assert np.all(np.imag(F)==0) #np.imag(F)ï¼šæå–å¤æ•°çš„è™šéƒ¨ï¼›np.real(F)ï¼šæå–å¤æ•°çš„å®éƒ¨ 
                      #np.any()ï¼Œnp.all()ï¼Œæ¥åˆ¤æ–­ä¸€ä¸ªçŸ©é˜µä¸­æ˜¯å¦æœ‰0å…ƒç´ æˆ–ä¸€ä¸ªçŸ©é˜µæ˜¯å¦æ˜¯é›¶çŸ©é˜µ
F = np.real(F)
# 1.1.æ ¹æ®Yå’ŒFåšçº¿æ€§å›å½’ï¼Œæ±‚Y_f_pred
#ã€å‚è€ƒï¼šhttps://www.cnblogs.com/learnbydoing/p/12190168.html
from sklearn.linear_model import LinearRegression
model_Y_f = LinearRegression()
model_Y_f = model_Y_f.fit(F, Y) #np.real(F)ï¼šæå–å¤æ•°çš„å®éƒ¨
'''
# get result: y = b0 + b1x
r_sq = model_Y_f.score(F, Y)
print('coefficient of determination(ğ‘…Â²) :', r_sq)
# coefficient of determination(ğ‘…Â²) : 0.715875613747954
print('intercept:', model_Y_f.intercept_)
# ï¼ˆæ ‡é‡ï¼‰ ç³»æ•°b0 intercept: 5.633333333333329 -------this will be an array when y is also 2-dimensional
print('slope:', model_Y_f.coef_)
# ï¼ˆæ•°ç»„ï¼‰æ–œç‡b1 slope: [0.54]        ---------this will be 2-d array when y is also 2-dimensional
'''
# predict response: given x, get y from the model y = b0+b1x
Y_f_pred = model_Y_f.predict(F)
# 1.2.æ±‚æ®‹å·®Y_u
Y_u = Y - Y_f_pred
Y_u.shape

# 2.æ±‚Y_uä¸Uçš„ç›¸å…³ç³»æ•°ï¼Œç„¶åæŒ‰ç…§è®ºæ–‡ï¼Œæ ¹æ®$S = {j:|corr(U_j,Y_u)|>alpha & {j:k_j>=kappa}$ ï¼Œè¿™é‡Œalphaå’Œkappaæ˜¯è¶…å‚æ•°
#   éœ€è¦æ³¨æ„çš„æ˜¯ï¼šå› ä¸ºU_jæ¥è‡ªäºD_freqï¼Œæ‰€ä»¥å·²ç»æ»¡è¶³{j:k_j>=kappa}äº†ã€‚ 
#   å¦å¤–ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬ä¼šç›´æ¥å–ç›¸å…³åº¦ç»å¯¹å€¼æ’å‰alphaçš„U_jï¼Œæ‰€ä»¥ä¸‹é¢å‡ºç°çš„alphaå’Œä¸Šè¿°æåˆ°è®ºæ–‡é‡Œå«ä¹‰ç¨æœ‰åŒºåˆ«ï¼Œä¸è¿‡ä½œç”¨æ˜¯ä¸€æ ·çš„ã€‚

# ã€å¸Œè…Šå­—æ¯ä¸è‹±æ–‡å¯¹ç…§ï¼šhttps://blog.csdn.net/bxlover007/article/details/105725593
#ã€æ±‚ç›¸å…³ç³»æ•°ï¼šhttps://www.cnblogs.com/marszhw/p/12175454.html
import random
import numpy as np
# åˆ¤æ–­Uçš„æ¯ä¸ªå¤æ•°å…ƒç´ æ˜¯å¦ä¸ºå®æ•°ï¼š
assert np.all(np.imag(U)==0) #np.imag(U)ï¼šæå–å¤æ•°çš„è™šéƒ¨ï¼›np.real(U)ï¼šæå–å¤æ•°çš„å®éƒ¨ 
                      #np.any()ï¼Œnp.all()ï¼Œæ¥åˆ¤æ–­ä¸€ä¸ªçŸ©é˜µä¸­æ˜¯å¦æœ‰0å…ƒç´ æˆ–ä¸€ä¸ªçŸ©é˜µæ˜¯å¦æ˜¯é›¶çŸ©é˜µ
U = np.real(U)
corr_U_j__Y_u = np.zeros(len(D__freq))
for j in range(len(D__freq)):
  tmp = np.array([Y_u, U[:,j]])
  corr_U_j__Y_u[j] = np.corrcoef(tmp)[0][1] # np.cov(tmp) # è®¡ç®—åæ–¹å·®çŸ©é˜µ ; np.corrcoef(tmp) # è®¡ç®—ç›¸å…³ç³»æ•°çŸ©é˜µ
corr_abs_U_j__Y_u = abs(corr_U_j__Y_u)
# ä¸‹ä¸€æ­¥å°±æ˜¯å–å‰alphaå¤§çš„ç›¸å…³åº¦ç»å¯¹å€¼å¯¹åº”çš„U_jçš„ä¸‹æ ‡ï¼š
sorted_indices_U = np.argsort(corr_abs_U_j__Y_u)
sorted_indices_U #ä¸Šé¢å¾—åˆ°çš„sorted_indices_Uå°±æ˜¯ç›¸å…³åº¦ç»å¯¹å€¼æ’åºåçš„ç»“æœï¼Œå·§å¦™çš„æ˜¯è¿™é‡Œæ˜¯ç”¨æ•°ç»„ä¸‹æ ‡æ¥è¡¨ç¤ºçš„ï¼Œä¹Ÿå°±æ˜¯è¯´å…¶ä¸­å­˜æ”¾çš„æ˜¯ç›¸å…³åº¦ç»å¯¹å€¼ç”±å°åˆ°å¤§çš„é¡ºåºæ’åºæ—¶å¯¹åº”çš„ä¸‹æ ‡[0, 4, 1, 3, 2]ï¼Œè€Œä¸æ˜¯ç›´æ¥å­˜æ”¾ç›¸å…³åº¦ç»å¯¹å€¼ã€‚
# print("Is min equals:",min(corr_abs_U_j__Y_u)==corr_abs_U_j__Y_u[sorted_indices_U[0]]) # éªŒè¯æ’åºæ­£ç¡®
alpha = 1000
topk_U_j = sorted_indices_U[:-alpha-1:-1]
# æœ€åå¾—åˆ°æƒ…æ„Ÿè¯æ±‡é›†S
S = [k for k,v in D__freq.items() if v in topk_U_j]

print(S)

assert len(S) == 1000
len(D__freq)

# ç”ŸæˆU_Sï¼ˆå¯¹åº”è®ºæ–‡ä¸­U_i,S)
U_S = U[:,topk_U_j]

# ä¿å­˜æ•°æ®ï¼š
np.save("F.npy",F)
np.save("U_S.npy",U_S)
np.save("Y.npy",np.array(data['pct_chg']))

# è¯»å–æ•°æ®
import numpy as np

F = np.load("F.npy")
U_S = np.load("U_S.npy")
Y = np.load("Y.npy")
print("æ€»å…±æœ‰ ",len(Y)," æ¡æ•°æ®")

word_num , doc_num = len(D__freq), len(documents)
S_word_num = len(S)
assert X.shape == (doc_num, word_num)
assert F.shape == (doc_num, k)
assert B.shape == (word_num, k)
assert U.shape == (doc_num, word_num)
assert U_S.shape == (doc_num, S_word_num)

# æ¨¡å‹è®­ç»ƒ
# tf2å®ç°çº¿æ€§å›å½’ï¼šhttps://blog.csdn.net/Mrwxxxx/article/details/108133137
import tensorflow as tf
import matplotlib.pyplot as plt


# æ ·æœ¬å‡†å¤‡
# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
from sklearn.model_selection import train_test_split
test_size = 0.1
random_state = 7600
F_train, F_test, U_S_train, U_S_test, Y_train, Y_test= train_test_split(F, U_S, np.array(data['pct_chg']), test_size=test_size, random_state=random_state)
# U_test.shape: (589, 8423)

#å‚æ•°
a = tf.Variable(tf.zeros([1], name='a'))
b = tf.Variable(tf.random.uniform([1,k], -1.0, 1.0), name='b')
BETA = tf.Variable(tf.random.uniform([1,S_word_num], -1.0, 1.0), name='BETA')

#éœ€è¦æ›´æ–°çš„å˜é‡
variables = [a, b, BETA]

#é‡‡ç”¨æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–å‚æ•°
#ä¼˜åŒ–å™¨
opt = tf.keras.optimizers.SGD(1e-3)
#è¿­ä»£æ¬¡æ•°
num_epoch = 5
for e in range(num_epoch):
  loss = 0
  for i in range(len(F_train)):
    #ä½¿ç”¨tf.GradientTape()è®°å½•æŸå¤±å‡½æ•°çš„æ¢¯åº¦ä¿¡æ¯
    with tf.GradientTape() as tape:
        #é¢„æµ‹å€¼
        y = tf.matmul(tf.cast(b ,tf.float64),tf.expand_dims(F_train[i,:],axis=0),transpose_b=True) + tf.matmul(tf.cast(BETA ,tf.float64),tf.expand_dims(U_S_train[i,:],axis=0),transpose_b=True) + tf.cast(a ,tf.float64) # w * x_data + b
        #æŸå¤±å€¼
        loss += tf.reduce_mean(tf.square(y - Y_train[i]), name='loss') + tf.cast(tf.reduce_sum(tf.square(BETA)),tf.double)
  #è‡ªåŠ¨è®¡ç®—loss å…³äºæ›´æ–°å˜é‡çš„æ¢¯åº¦
  grads = tape.gradient(loss, variables)
  #è‡ªåŠ¨æ ¹æ®æ¢¯åº¦æ›´æ–°å‚æ•°
  opt.apply_gradients(zip(grads, variables))
  if(e % 5 == 0):
      print(variables)

# æ¨¡å‹æµ‹è¯•
MSE = 0
Y_pred = []
for i in range(len(F_test)):
    y = tf.matmul(tf.cast(b ,tf.float64),tf.expand_dims(F_test[i,:],axis=0),transpose_b=True) + tf.matmul(tf.cast(BETA ,tf.float64),tf.expand_dims(U_S_test[i,:],axis=0),transpose_b=True) + tf.cast(a ,tf.float64) # w * x_data + b
    Y_pred.append(y)
    #æŸå¤±å€¼
    MSE += tf.reduce_mean(tf.square(y - Y_test[i]))
print("test MSE: ",MSE/len(Y_test))

list(Y_test)[:10]

Y_pred[:10]

np.append(F_train,U_S_train)

# æ¨¡å‹è®­ç»ƒ
# lassoå®ç°ï¼š
# lassoåŸç†ï¼šhttps://zhuanlan.zhihu.com/p/116869931
from sklearn.linear_model import Lasso
model_lasso = Lasso()
model_lasso = model_lasso.fit(np.append(F_train,U_S_train).reshape((-1,k+S_word_num)), Y_train.reshape((-1,1))) #np.real(F)ï¼šæå–å¤æ•°çš„å®éƒ¨
'''
# get result: y = b0 + b1x
r_sq = model_Y_f.score(F, Y)
print('coefficient of determination(ğ‘…Â²) :', r_sq)
# coefficient of determination(ğ‘…Â²) : 0.715875613747954
print('intercept:', model_Y_f.intercept_)
# ï¼ˆæ ‡é‡ï¼‰ ç³»æ•°b0 intercept: 5.633333333333329 -------this will be an array when y is also 2-dimensional
print('slope:', model_Y_f.coef_)
# ï¼ˆæ•°ç»„ï¼‰æ–œç‡b1 slope: [0.54]        ---------this will be 2-d array when y is also 2-dimensional
'''
# predict response: given x, get y from the model y = b0+b1x
Y_lasso_pred = model_lasso.predict(np.append(F_test,U_S_test).reshape((-1,k+S_word_num)))

Y_lasso_pred.shape

from sklearn.metrics import mean_squared_error
mean_squared_error(Y_test,Y_lasso_pred)  #è™½ç„¶æ¯”tf2çš„ç‰ˆæœ¬å°ï¼Œä½†æ˜¯æ‰€æœ‰é¢„æµ‹å€¼éƒ½æ˜¯ä¸€æ‘¸ä¸€æ ·ã€‚ã€‚ã€‚

Y_lasso_pred