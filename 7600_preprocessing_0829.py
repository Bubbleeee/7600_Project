# -*- coding: utf-8 -*-
"""7600_preprocessing_0829.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13Mslhla0M7b0jiJcjpwSAFLJ-ZzwClxR
"""

import pandas as pd
import jieba

from google.colab import drive

drive.mount('/content/drive')  ## https://blog.csdn.net/sdaujz/article/details/106531632
                 ## https://blog.csdn.net/sdaujz/article/details/106531632

cd /content/drive/MyDrive/Capstone_Project/data

data = pd.read_csv("0827_Cecilia.csv")
data.head(3)

data['Article'][0]

# jieba分词的demo【可跳过直接下一步】
import jieba
# 载入自定义词库（比如：所有股票名称/代码
# jieba.load_userdict("./dict/jieba_dict.txt")

jieba.enable_paddle()# 启动paddle模式。 0.40版之后开始支持，早期版本不支持

seg_list = jieba.cut("我来到北京清华大学",use_paddle=True) # 使用paddle模式
print("Paddle Mode: " + '/'.join(list(seg_list)))

seg_list = jieba.cut("我来到北京清华大学", cut_all=True)
print("Full Mode: " + "/ ".join(seg_list))  # 全模式

seg_list = jieba.cut("我来到北京清华大学", cut_all=False)
print("Default Mode: " + "/ ".join(seg_list))  # 精确模式

seg_list = jieba.cut("他来到了网易杭研大厦")  # 默认是精确模式
print(", ".join(seg_list))

seg_list = jieba.cut_for_search("小明硕士毕业于中国科学院计算所，后在日本京都大学深造")  # 搜索引擎模式
print(", ".join(seg_list))

# 缺失值处理
# 缺失值查找与填充：https://blog.csdn.net/qq_28811329/article/details/82821971?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control
import math 
# 删除含有缺失值的行：https://blog.csdn.net/Guo_ya_nan/article/details/81042882 或者 https://www.delftstack.com/zh/howto/python-pandas/pandas-drop-rows-with-nan/#:~:text=Pandas%20%E4%BD%BF%E7%94%A8%20DataFrame.dropna%20%28%29%20%E6%96%B9%E6%B3%95%E5%8F%AA%E5%88%A0%E9%99%A4%E6%89%80%E6%9C%89%E5%88%97%E9%83%BD%E6%98%AF%20NaN%20%E5%80%BC%E7%9A%84%E8%A1%8C.%20%E5%AE%83%E5%8F%AA%E5%88%A0%E9%99%A4,%E6%88%91%E4%BB%AC%E5%9C%A8%20dropna%20%28%29%20%E6%96%B9%E6%B3%95%E4%B8%AD%E8%AE%BE%E7%BD%AE%20how%3D%27all%27%20%EF%BC%8C%E8%AE%A9%E8%AF%A5%E6%96%B9%E6%B3%95%E5%8F%AA%E5%9C%A8%E8%A1%8C%E7%9A%84%E6%89%80%E6%9C%89%E5%88%97%E5%80%BC%E9%83%BD%E6%98%AF%20NaN%20%E6%97%B6%E6%89%8D%E5%88%A0%E9%99%A4%E8%A1%8C%E3%80%82.
data_copy = data.copy()
data_copy.dropna(axis=0, how='any', inplace=True)
print("删除前：",len(data))
print("删除后：",len(data_copy))
# 删除目标值Y（pct_chg）缺失的行
def is_not_nan(x):
  return not math.isnan(x)
data_copy = data_copy[data_copy['pct_chg'].map(is_not_nan)]
print("删除前：",len(data))
print("删除后：",len(data_copy))

data = data_copy.reset_index()

#删除固定banner位
for i in range(0,len(data)):
    if '金麒麟' in data['Article'][i]:
        data['Article'][i] = data['Article'][i][40:9999999]

#删除Y（pct_chg）缺失的行


def getJiebaList(x):
  # 对字符串'['string1','string2']'结巴分词¶，得到list：[[string1 word],[string2 word]]
  result = []
  y = x.strip("[").strip("]").split(", ")
  for string in y:
    tmp = " ".join(jieba.cut(string.strip("\'").strip('\\u3000\\u3000').strip('\u3000\u3000'))).split()
    result.append(tmp)
  return result

def transListsToList(x):
  # 将包含多个lists的list x合并为1个list(即一篇文章的词汇存在1个list中)
  return [i for p in x for i in p]


data['doc_Jieba_List']=data.Article.apply(getJiebaList)
data['doc_Jieba']=data.doc_Jieba_List.apply(transListsToList)
data.head(1)

for i in range(len(data['Article'])):
  if '投资收益' in data['Article'][i]:
    print(data['doc_Jieba'][i],'\n')
    print(data['URL'][i])
    break

def get_custom_stopwords(stop_words_file):
    with open(stop_words_file) as f:
        stopwords = f.read()
    stopwords_list = stopwords.split('\n')
    custom_stopwords_list = [i for i in stopwords_list]
    return custom_stopwords_list

stop_words_file = './stopwords-master/哈工大停用词表.txt'
stopwords_1 = get_custom_stopwords(stop_words_file)
stop_words_file = './stopwords-master/中文停用词表.txt'
stopwords_2 = get_custom_stopwords(stop_words_file)
stop_words_file = './stopwords-master/四川大学机器智能实验室停用词库.txt'
stopwords_3 = get_custom_stopwords(stop_words_file)
stop_words_file = './stopwords-master/百度停用词表.txt'
stopwords_4 = get_custom_stopwords(stop_words_file)
stopwords=stopwords_1+stopwords_2+stopwords_3+stopwords_4
stopwords=list(set(stopwords))
#去重后2318个停用词

def clean_stopword(x):
    y=[]
    for i in x:
        if i not in stopwords:
            y.append(i)
    return y

data['doc_Jieba_2']=data.doc_Jieba.apply(clean_stopword)

# Selecting frequently-used words：选择出现次数大于kappa的词汇（kappa为超参数），对应论文中D__freq
## 计算包含词汇i的文章数，存在字典dict_word_ariticles中；然后计算每篇文章的词汇set set_word_ariticles，
dict_word_ariticles = {}
for article_list in data['doc_Jieba_2']:
  set_word_ariticles = set(article_list)
  for word in list(set_word_ariticles):
    if word in dict_word_ariticles:
      dict_word_ariticles[word] += 1
    else:
      dict_word_ariticles[word] = 1
# dict_word_ariticles


## 选择出现次数大于kappa的词汇（kappa为超参数）
kappa = 20
frequent_dict = [k for k, v in dict_word_ariticles.items() if v > kappa]

# 词到ID的映射, 使得每个词有一个ID
D__freq = dict(zip(frequent_dict, range(len(frequent_dict))))
# sorted(D__freq)
len(D__freq)

tmp_num = 0
for i in range(0,len(data)):
    if '收益' in data['Article'][i]:
        tmp_num += 1
print(tmp_num)
tmp_num = 0
for i in range(0,len(data)):
    if '收益' in data['doc_Jieba_2'][i]:
        tmp_num += 1
print(tmp_num)
print(dict_word_ariticles['收益'] + dict_word_ariticles['收益率'] + dict_word_ariticles['投资收益'] + dict_word_ariticles['收益分配'] + dict_word_ariticles['经济收益'] + dict_word_ariticles['收益权'] + dict_word_ariticles['低收益'])
for k,v in dict_word_ariticles.items():
  if '收益' in k:
    print(k,'\n')
dict_word_ariticles['收益']

"""### Factor modeling【todo】
$ X = FB^T + U$

### Learning factors and idiosyncratic components【todo】
（0）求X：

（1.1）指定k，求F：$F=sqrt(n)*(XX^T的最大的前k个特征值）$

（1.2）求出k，再求F

（2） 根据X和F，求B：$B=X^TF/n$

（3）根据X，F和B，求U：$U=X-FB^T$
"""

# sorted(D__freq.items(), key=lambda item:item[1])

# （0）求X
from collections import Counter
from itertools import chain
import numpy as np
documents = data['doc_Jieba_2']
def word_matrix(docs, dictionary, word_num, doc_num ):
    '''计算词频矩阵：行表示文档，列表示词语'''
    # 词到ID的映射, 使得每个词有一个ID
    # dictionary = dict(zip(words, range(len(words))))
    #print(dictionary)
    # 创建一个空的矩阵, 行数等于文档数, 列数等于词数
    matrix = np.zeros((doc_num,  word_num))
    # 逐个文档统计词频
    for row in range(doc_num):  # row 表示矩阵第几行，即第几个文档
        # 统计词频
        count = Counter(docs[row])#其实是个词典，词典元素为：{单词：次数}。
        for word in count:
          if word in D__freq:
            # 用word的id表示word在矩阵中的列数，该文档表示行数。
            col= dictionary[word] # col表示矩阵第几列，即第几个单词
            # 把词频赋值给矩阵
            matrix[row,col] = count[word]
    return matrix
 
X = word_matrix(documents, D__freq, len(D__freq), len(documents))
print("Word frequency matrix is:",X) ## [doc_num, word_num]

# （1.1）指定k，求F：$F=sqrt(n)*(XX^T的最大的前k个特征值）$。【todo】检查下算得对不对 ；【todo】这里k是超参数
# 【参考：https://blog.csdn.net/w450468524/article/details/54885773
import numpy as np
n = len(documents)
k = 10

eigenvalue, featurevector = np.linalg.eig(np.matmul(X,X.T))
sorted_indices = np.argsort(eigenvalue)
# 上面得到的sorted_indices就是特征值排序后的结果，巧妙的是这里是用数组下标来表示的，也就是说其中存放的是特征值由小到大的顺序排序时对应的下标[0, 4, 1, 3, 2]，而不是直接存放特征值。
# 下一步就是取前k大的特征向量了：
topk_evecs = featurevector[:,sorted_indices[:-k-1:-1]]

F = np.sqrt(n) * topk_evecs  ## [doc_num, k]
print("F = ",F)

# 复数：
# 实数部分和虚数部分都是浮点数
# 虚数部分必须有j或J
# 下面是些得数：
# 64.23+1j            4.34-8.5j            0.23-8.33j            1.23e-0.45+6.5e+0.83j              -1.23-3.5j            -0.34-0j
# 复数中的内建属性

# （1.2）求出k，再求F：

# （2） 根据X和F，求B：$B=X^TF/n$
B = np.matmul(X.T,F) / n
print("B =",B)  ## [word_num, k]
# （3）根据X，F和B，求U：$U=X-FB^T$
U = X - np.matmul(F,B.T)
print("U =",U) ## [doc_num, word_num]

word_num , doc_num = len(D__freq), len(documents)
assert X.shape == (doc_num, word_num)
assert F.shape == (doc_num, k)
assert B.shape == (word_num, k)
assert U.shape == (doc_num, word_num)

"""### Learning conditional sentiment-charged words S【todo】"""

## 对于Y缺失值，先用平均数填充，之后确定了Y，要在一开始就把包含缺失Y值的data的整行数据删除
data.open = data.open.fillna(data.open.mean())

data.open

# 1.根据Y和F做线性回归，求残差Y_u
# 先用收盘价（open）作为Y
from numpy import random
Y = data.open
# 判断F的每个复数元素是否为实数：
assert np.all(np.imag(F)==0) #np.imag(F)：提取复数的虚部；np.real(F)：提取复数的实部 
                      #np.any()，np.all()，来判断一个矩阵中是否有0元素或一个矩阵是否是零矩阵
F = np.real(F)
# 1.1.根据Y和F做线性回归，求Y_f_pred
#【参考：https://www.cnblogs.com/learnbydoing/p/12190168.html
from sklearn.linear_model import LinearRegression
model_Y_f = LinearRegression()
model_Y_f = model_Y_f.fit(F, Y) #np.real(F)：提取复数的实部
'''
# get result: y = b0 + b1x
r_sq = model_Y_f.score(F, Y)
print('coefficient of determination(𝑅²) :', r_sq)
# coefficient of determination(𝑅²) : 0.715875613747954
print('intercept:', model_Y_f.intercept_)
# （标量） 系数b0 intercept: 5.633333333333329 -------this will be an array when y is also 2-dimensional
print('slope:', model_Y_f.coef_)
# （数组）斜率b1 slope: [0.54]        ---------this will be 2-d array when y is also 2-dimensional
'''
# predict response: given x, get y from the model y = b0+b1x
Y_f_pred = model_Y_f.predict(F)
# 1.2.求残差Y_u
Y_u = Y - Y_f_pred
Y_u.shape

# 2.求Y_u与U的相关系数，然后按照论文，根据$S = {j:|corr(U_j,Y_u)|>alpha & {j:k_j>=kappa}$ ，这里alpha和kappa是超参数
#   需要注意的是：因为U_j来自于D_freq，所以已经满足{j:k_j>=kappa}了。 
#   另外，接下来我们会直接取相关度绝对值排前alpha的U_j，所以下面出现的alpha和上述提到论文里含义稍有区别，不过作用是一样的。

# 【希腊字母与英文对照：https://blog.csdn.net/bxlover007/article/details/105725593
#【求相关系数：https://www.cnblogs.com/marszhw/p/12175454.html
import random
import numpy as np
# 判断U的每个复数元素是否为实数：
assert np.all(np.imag(U)==0) #np.imag(U)：提取复数的虚部；np.real(U)：提取复数的实部 
                      #np.any()，np.all()，来判断一个矩阵中是否有0元素或一个矩阵是否是零矩阵
U = np.real(U)
corr_U_j__Y_u = np.zeros(len(D__freq))
for j in range(len(D__freq)):
  tmp = np.array([Y_u, U[:,j]])
  corr_U_j__Y_u[j] = np.corrcoef(tmp)[0][1] # np.cov(tmp) # 计算协方差矩阵 ; np.corrcoef(tmp) # 计算相关系数矩阵
corr_abs_U_j__Y_u = abs(corr_U_j__Y_u)
# 下一步就是取前alpha大的相关度绝对值对应的U_j的下标：
sorted_indices_U = np.argsort(corr_abs_U_j__Y_u)
sorted_indices_U #上面得到的sorted_indices_U就是相关度绝对值排序后的结果，巧妙的是这里是用数组下标来表示的，也就是说其中存放的是相关度绝对值由小到大的顺序排序时对应的下标[0, 4, 1, 3, 2]，而不是直接存放相关度绝对值。
# print("Is min equals:",min(corr_abs_U_j__Y_u)==corr_abs_U_j__Y_u[sorted_indices_U[0]]) # 验证排序正确
alpha = 1000
topk_U_j = sorted_indices_U[:-alpha-1:-1]
# 最后得到情感词汇集S
S = [k for k,v in D__freq.items() if v in topk_U_j]

print(S)

assert len(S) == 1000
len(D__freq)

# 生成U_S（对应论文中U_i,S)
U_S = U[:,topk_U_j]

# 保存数据：
np.save("F.npy",F)
np.save("U_S.npy",U_S)
np.save("Y.npy",np.array(data['pct_chg']))

# 读取数据
import numpy as np

F = np.load("F.npy")
U_S = np.load("U_S.npy")
Y = np.load("Y.npy")
print("总共有 ",len(Y)," 条数据")

word_num , doc_num = len(D__freq), len(documents)
S_word_num = len(S)
assert X.shape == (doc_num, word_num)
assert F.shape == (doc_num, k)
assert B.shape == (word_num, k)
assert U.shape == (doc_num, word_num)
assert U_S.shape == (doc_num, S_word_num)

# 模型训练
# tf2实现线性回归：https://blog.csdn.net/Mrwxxxx/article/details/108133137
import tensorflow as tf
import matplotlib.pyplot as plt


# 样本准备
# 划分训练集和测试集
from sklearn.model_selection import train_test_split
test_size = 0.1
random_state = 7600
F_train, F_test, U_S_train, U_S_test, Y_train, Y_test= train_test_split(F, U_S, np.array(data['pct_chg']), test_size=test_size, random_state=random_state)
# U_test.shape: (589, 8423)

#参数
a = tf.Variable(tf.zeros([1], name='a'))
b = tf.Variable(tf.random.uniform([1,k], -1.0, 1.0), name='b')
BETA = tf.Variable(tf.random.uniform([1,S_word_num], -1.0, 1.0), name='BETA')

#需要更新的变量
variables = [a, b, BETA]

#采用梯度下降法优化参数
#优化器
opt = tf.keras.optimizers.SGD(1e-3)
#迭代次数
num_epoch = 5
for e in range(num_epoch):
  loss = 0
  for i in range(len(F_train)):
    #使用tf.GradientTape()记录损失函数的梯度信息
    with tf.GradientTape() as tape:
        #预测值
        y = tf.matmul(tf.cast(b ,tf.float64),tf.expand_dims(F_train[i,:],axis=0),transpose_b=True) + tf.matmul(tf.cast(BETA ,tf.float64),tf.expand_dims(U_S_train[i,:],axis=0),transpose_b=True) + tf.cast(a ,tf.float64) # w * x_data + b
        #损失值
        loss += tf.reduce_mean(tf.square(y - Y_train[i]), name='loss') + tf.cast(tf.reduce_sum(tf.square(BETA)),tf.double)
  #自动计算loss 关于更新变量的梯度
  grads = tape.gradient(loss, variables)
  #自动根据梯度更新参数
  opt.apply_gradients(zip(grads, variables))
  if(e % 5 == 0):
      print(variables)

# 模型测试
MSE = 0
Y_pred = []
for i in range(len(F_test)):
    y = tf.matmul(tf.cast(b ,tf.float64),tf.expand_dims(F_test[i,:],axis=0),transpose_b=True) + tf.matmul(tf.cast(BETA ,tf.float64),tf.expand_dims(U_S_test[i,:],axis=0),transpose_b=True) + tf.cast(a ,tf.float64) # w * x_data + b
    Y_pred.append(y)
    #损失值
    MSE += tf.reduce_mean(tf.square(y - Y_test[i]))
print("test MSE: ",MSE/len(Y_test))

list(Y_test)[:10]

Y_pred[:10]

np.append(F_train,U_S_train)

# 模型训练
# lasso实现：
# lasso原理：https://zhuanlan.zhihu.com/p/116869931
from sklearn.linear_model import Lasso
model_lasso = Lasso()
model_lasso = model_lasso.fit(np.append(F_train,U_S_train).reshape((-1,k+S_word_num)), Y_train.reshape((-1,1))) #np.real(F)：提取复数的实部
'''
# get result: y = b0 + b1x
r_sq = model_Y_f.score(F, Y)
print('coefficient of determination(𝑅²) :', r_sq)
# coefficient of determination(𝑅²) : 0.715875613747954
print('intercept:', model_Y_f.intercept_)
# （标量） 系数b0 intercept: 5.633333333333329 -------this will be an array when y is also 2-dimensional
print('slope:', model_Y_f.coef_)
# （数组）斜率b1 slope: [0.54]        ---------this will be 2-d array when y is also 2-dimensional
'''
# predict response: given x, get y from the model y = b0+b1x
Y_lasso_pred = model_lasso.predict(np.append(F_test,U_S_test).reshape((-1,k+S_word_num)))

Y_lasso_pred.shape

from sklearn.metrics import mean_squared_error
mean_squared_error(Y_test,Y_lasso_pred)  #虽然比tf2的版本小，但是所有预测值都是一摸一样。。。

Y_lasso_pred